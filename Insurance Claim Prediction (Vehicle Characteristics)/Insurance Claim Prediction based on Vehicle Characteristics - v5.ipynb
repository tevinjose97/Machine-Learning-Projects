{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,RandomForestClassifier,GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,PredefinedSplit\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train dataset\n",
    "insurance_data=pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_ID</th>\n",
       "      <th>Household_ID</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Calendar_Year</th>\n",
       "      <th>Model_Year</th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>NVVar1</th>\n",
       "      <th>NVVar2</th>\n",
       "      <th>NVVar3</th>\n",
       "      <th>NVVar4</th>\n",
       "      <th>Claim_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.265666e+06</td>\n",
       "      <td>3.415164e+06</td>\n",
       "      <td>1.869133</td>\n",
       "      <td>2006.118667</td>\n",
       "      <td>1999.505933</td>\n",
       "      <td>-0.006781</td>\n",
       "      <td>-0.063997</td>\n",
       "      <td>-0.025057</td>\n",
       "      <td>-0.053640</td>\n",
       "      <td>0.012070</td>\n",
       "      <td>-0.044910</td>\n",
       "      <td>-0.024929</td>\n",
       "      <td>-0.064832</td>\n",
       "      <td>0.032385</td>\n",
       "      <td>0.071626</td>\n",
       "      <td>0.078748</td>\n",
       "      <td>0.041319</td>\n",
       "      <td>54.862481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.034083e+06</td>\n",
       "      <td>1.942246e+06</td>\n",
       "      <td>1.150848</td>\n",
       "      <td>0.804947</td>\n",
       "      <td>5.048889</td>\n",
       "      <td>0.976902</td>\n",
       "      <td>0.960495</td>\n",
       "      <td>1.016570</td>\n",
       "      <td>0.959304</td>\n",
       "      <td>0.986830</td>\n",
       "      <td>0.971651</td>\n",
       "      <td>1.000754</td>\n",
       "      <td>0.979235</td>\n",
       "      <td>1.063700</td>\n",
       "      <td>1.148950</td>\n",
       "      <td>1.144502</td>\n",
       "      <td>1.073275</td>\n",
       "      <td>244.230985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.160000e+03</td>\n",
       "      <td>6.830000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>1981.000000</td>\n",
       "      <td>-2.578222</td>\n",
       "      <td>-2.441519</td>\n",
       "      <td>-2.744055</td>\n",
       "      <td>-2.457475</td>\n",
       "      <td>-3.350344</td>\n",
       "      <td>-2.300626</td>\n",
       "      <td>-2.262411</td>\n",
       "      <td>-1.882994</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.670346e+06</td>\n",
       "      <td>1.917093e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "      <td>1997.000000</td>\n",
       "      <td>-0.665897</td>\n",
       "      <td>-0.816152</td>\n",
       "      <td>-0.869687</td>\n",
       "      <td>-0.783019</td>\n",
       "      <td>-0.662235</td>\n",
       "      <td>-0.688765</td>\n",
       "      <td>-0.898486</td>\n",
       "      <td>-0.646298</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.255290e+06</td>\n",
       "      <td>3.654160e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>-0.320393</td>\n",
       "      <td>-0.124506</td>\n",
       "      <td>-0.221758</td>\n",
       "      <td>-0.106471</td>\n",
       "      <td>-0.115098</td>\n",
       "      <td>-0.241936</td>\n",
       "      <td>-0.468419</td>\n",
       "      <td>-0.269656</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.858568e+06</td>\n",
       "      <td>4.992703e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>2003.000000</td>\n",
       "      <td>0.442930</td>\n",
       "      <td>0.480684</td>\n",
       "      <td>0.726996</td>\n",
       "      <td>0.485509</td>\n",
       "      <td>0.550982</td>\n",
       "      <td>0.500830</td>\n",
       "      <td>0.870931</td>\n",
       "      <td>0.327962</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>5.905322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.051789e+07</td>\n",
       "      <td>6.484624e+06</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>4.540768</td>\n",
       "      <td>6.307799</td>\n",
       "      <td>5.563325</td>\n",
       "      <td>6.100857</td>\n",
       "      <td>3.869488</td>\n",
       "      <td>4.584289</td>\n",
       "      <td>4.127148</td>\n",
       "      <td>33.457737</td>\n",
       "      <td>6.627110</td>\n",
       "      <td>8.883081</td>\n",
       "      <td>8.691144</td>\n",
       "      <td>6.388802</td>\n",
       "      <td>11440.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Row_ID  Household_ID       Vehicle  Calendar_Year    Model_Year  \\\n",
       "count  3.000000e+04  3.000000e+04  30000.000000   30000.000000  30000.000000   \n",
       "mean   5.265666e+06  3.415164e+06      1.869133    2006.118667   1999.505933   \n",
       "std    3.034083e+06  1.942246e+06      1.150848       0.804947      5.048889   \n",
       "min    1.160000e+03  6.830000e+02      1.000000    2005.000000   1981.000000   \n",
       "25%    2.670346e+06  1.917093e+06      1.000000    2005.000000   1997.000000   \n",
       "50%    5.255290e+06  3.654160e+06      2.000000    2006.000000   2000.000000   \n",
       "75%    7.858568e+06  4.992703e+06      2.000000    2007.000000   2003.000000   \n",
       "max    1.051789e+07  6.484624e+06     17.000000    2007.000000   2008.000000   \n",
       "\n",
       "               Var1          Var2          Var3          Var4          Var5  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.006781     -0.063997     -0.025057     -0.053640      0.012070   \n",
       "std        0.976902      0.960495      1.016570      0.959304      0.986830   \n",
       "min       -2.578222     -2.441519     -2.744055     -2.457475     -3.350344   \n",
       "25%       -0.665897     -0.816152     -0.869687     -0.783019     -0.662235   \n",
       "50%       -0.320393     -0.124506     -0.221758     -0.106471     -0.115098   \n",
       "75%        0.442930      0.480684      0.726996      0.485509      0.550982   \n",
       "max        4.540768      6.307799      5.563325      6.100857      3.869488   \n",
       "\n",
       "               Var6          Var7          Var8        NVVar1        NVVar2  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean      -0.044910     -0.024929     -0.064832      0.032385      0.071626   \n",
       "std        0.971651      1.000754      0.979235      1.063700      1.148950   \n",
       "min       -2.300626     -2.262411     -1.882994     -0.231530     -0.266117   \n",
       "25%       -0.688765     -0.898486     -0.646298     -0.231530     -0.266117   \n",
       "50%       -0.241936     -0.468419     -0.269656     -0.231530     -0.266117   \n",
       "75%        0.500830      0.870931      0.327962     -0.231530     -0.266117   \n",
       "max        4.584289      4.127148     33.457737      6.627110      8.883081   \n",
       "\n",
       "             NVVar3        NVVar4  Claim_Amount  \n",
       "count  30000.000000  30000.000000  30000.000000  \n",
       "mean       0.078748      0.041319     54.862481  \n",
       "std        1.144502      1.073275    244.230985  \n",
       "min       -0.272337     -0.251419      0.000000  \n",
       "25%       -0.272337     -0.251419      0.000000  \n",
       "50%       -0.272337     -0.251419      0.000000  \n",
       "75%       -0.272337     -0.251419      5.905322  \n",
       "max        8.691144      6.388802  11440.750000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00000      21000\n",
       "35.95240         2\n",
       "36.90490         2\n",
       "143.28640        2\n",
       "32.29789         2\n",
       "             ...  \n",
       "129.48490        1\n",
       "255.52600        1\n",
       "48.25171         1\n",
       "565.83620        1\n",
       "39.52774         1\n",
       "Name: Claim_Amount, Length: 8997, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display frequency counts of values in 'Claim Amounts'\n",
    "insurance_data.Claim_Amount.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'Claim_Amount'}>]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "insurance_data.hist(column=['Claim_Amount'],bins=120,figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is observed that majority of the values are 0 claims\n",
    "#Various regressor models will be compared to find the best working model\n",
    "\n",
    "#Due to the large number of 0 claims, a tandem model will also be created/ tested by combining a binary classifier\n",
    "#to sort out the 0 and non 0 claims, and a regressor to predict the non zero claim values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a copy of the original dataset as a backup and work on it\n",
    "insurance_data_copy=insurance_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an extra column for identifying 0 and non 0 claims\n",
    "\n",
    "#Stratifier = 0 for 0 claims and 1 for non 0 Claims\n",
    "insurance_data_copy['Stratifier']=insurance_data_copy['Claim_Amount'].apply(lambda claim: claim if claim==0 else 1)\n",
    "\n",
    "#splitting training and test data\n",
    "original_train_set,original_test_set= train_test_split(insurance_data_copy,test_size=0.15,random_state=42)\n",
    "\n",
    "#Splitting train_2 and validation set proportionally\n",
    "train_set,validation_set=train_test_split(original_train_set,test_size=0.15,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique</th>\n",
       "      <th>Missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Blind_Submodel</th>\n",
       "      <td>1489</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blind_Model</th>\n",
       "      <td>787</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blind_Make</th>\n",
       "      <td>58</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NVCat</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat1</th>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrdCat</th>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat11</th>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat12</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat6</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat7</th>\n",
       "      <td>5</td>\n",
       "      <td>11910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat2</th>\n",
       "      <td>4</td>\n",
       "      <td>7668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat4</th>\n",
       "      <td>4</td>\n",
       "      <td>9359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat5</th>\n",
       "      <td>4</td>\n",
       "      <td>9370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat8</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat10</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cat9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Unique  Missing\n",
       "Blind_Submodel    1489       12\n",
       "Blind_Model        787       12\n",
       "Blind_Make          58       12\n",
       "NVCat               15        0\n",
       "Cat1                11       33\n",
       "OrdCat               8       18\n",
       "Cat3                 7        7\n",
       "Cat11                7       41\n",
       "Cat12                6        0\n",
       "Cat6                 6       33\n",
       "Cat7                 5    11910\n",
       "Cat2                 4     7668\n",
       "Cat4                 4     9359\n",
       "Cat5                 4     9370\n",
       "Cat8                 4        1\n",
       "Cat10                4        7\n",
       "Cat9                 2        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding out the useful features among the categorical features\n",
    "\n",
    "#Looping through all categorical columns to find the number of unique values and missing values\n",
    "\n",
    "missing=[]\n",
    "unique=[]\n",
    "col_names=[]\n",
    "\n",
    "for col in train_set.select_dtypes(exclude=['float64','int64']).columns:\n",
    "    values=set(train_set[col].values)\n",
    "    #only type of missing values are '?'\n",
    "    if \"?\" in values or \"\" in values:\n",
    "        missing.append((train_set[col]=='?').sum())\n",
    "    else:\n",
    "        missing.append(0)\n",
    "        \n",
    "    col_names.append(col)\n",
    "    unique.append(len(train_set[col].value_counts()))\n",
    "\n",
    "df_miss_uni=pd.DataFrame({'Unique':unique,'Missing':missing},index=col_names)\n",
    "df_miss_uni.sort_values('Unique',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose the categorical features that provide us with the most information (unique values), least missing values \n",
    "#and does not significantly increase our feature space (while Encoding)\n",
    "\n",
    "#From the above table, we choose the features Blind_Make,Blind_Model,Blind_Submodel,NVCat and OrdCat \n",
    "#(OrdCat is an ordinal variable ordered on integers)\n",
    "\n",
    "#In order to use the features Blind_Make,Blind_Model and Blind_Submodel with large number of unique values, \n",
    "#we do not use OneHotEncoder on them, we need to find an alternative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above table we see that Blind_Make, Blind_Model and Blind_Submodel have missing values, let's review them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_ID</th>\n",
       "      <th>Household_ID</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Calendar_Year</th>\n",
       "      <th>Model_Year</th>\n",
       "      <th>Blind_Make</th>\n",
       "      <th>Blind_Model</th>\n",
       "      <th>Blind_Submodel</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>NVCat</th>\n",
       "      <th>NVVar1</th>\n",
       "      <th>NVVar2</th>\n",
       "      <th>NVVar3</th>\n",
       "      <th>NVVar4</th>\n",
       "      <th>Claim_Amount</th>\n",
       "      <th>Stratifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21430</th>\n",
       "      <td>2959699</td>\n",
       "      <td>2063141</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.337415</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>4906216</td>\n",
       "      <td>3108586</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082855</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.429994</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23203</th>\n",
       "      <td>2668985</td>\n",
       "      <td>1916604</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.106594</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>8092873</td>\n",
       "      <td>5153020</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>N</td>\n",
       "      <td>2.054683</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>5.748932</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12305</th>\n",
       "      <td>8017601</td>\n",
       "      <td>5126989</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20116</th>\n",
       "      <td>4160788</td>\n",
       "      <td>2608193</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>8869889</td>\n",
       "      <td>5803719</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18735</th>\n",
       "      <td>4055236</td>\n",
       "      <td>2573687</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>1652478</td>\n",
       "      <td>939349</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>3706292</td>\n",
       "      <td>2448524</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>2710151</td>\n",
       "      <td>1930600</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>79.588900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17039</th>\n",
       "      <td>2077526</td>\n",
       "      <td>1240636</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.723858</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_ID  Household_ID  Vehicle  Calendar_Year  Model_Year Blind_Make  \\\n",
       "21430  2959699       2063141        1           2006        2004          ?   \n",
       "6606   4906216       3108586        1           2005        2002          ?   \n",
       "23203  2668985       1916604        1           2005        2003          ?   \n",
       "17886  8092873       5153020        1           2005        2003          ?   \n",
       "12305  8017601       5126989        1           2007        2002          ?   \n",
       "20116  4160788       2608193        1           2007        2004          ?   \n",
       "19932  8869889       5803719        2           2006        2003          ?   \n",
       "18735  4055236       2573687        1           2005        2002          ?   \n",
       "16665  1652478        939349        2           2007        2002          ?   \n",
       "20602  3706292       2448524        1           2005        2002          ?   \n",
       "15149  2710151       1930600        1           2007        2004          ?   \n",
       "17039  2077526       1240636        1           2006        2004          ?   \n",
       "\n",
       "      Blind_Model Blind_Submodel Cat1 Cat2  ...      Var6      Var7      Var8  \\\n",
       "21430           ?              ?    F    C  ...  0.498491 -0.972211  2.337415   \n",
       "6606            ?              ?    A    C  ... -0.082855 -0.677309  1.429994   \n",
       "23203           ?              ?    F    C  ...  0.059850 -0.972211  2.106594   \n",
       "17886           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "12305           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "20116           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "19932           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "18735           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "16665           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "20602           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "15149           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "17039           ?              ?    B    C  ... -0.211523 -0.800185  0.723858   \n",
       "\n",
       "      NVCat    NVVar1    NVVar2    NVVar3    NVVar4 Claim_Amount Stratifier  \n",
       "21430     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "6606      M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "23203     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "17886     N  2.054683 -0.266117 -0.272337 -0.251419     5.748932        1.0  \n",
       "12305     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20116     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "19932     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "18735     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "16665     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20602     N -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "15149     M -0.231530 -0.266117 -0.272337 -0.251419    79.588900        1.0  \n",
       "17039     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "\n",
       "[12 rows x 36 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[train_set['Blind_Make']=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_ID</th>\n",
       "      <th>Household_ID</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Calendar_Year</th>\n",
       "      <th>Model_Year</th>\n",
       "      <th>Blind_Make</th>\n",
       "      <th>Blind_Model</th>\n",
       "      <th>Blind_Submodel</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>NVCat</th>\n",
       "      <th>NVVar1</th>\n",
       "      <th>NVVar2</th>\n",
       "      <th>NVVar3</th>\n",
       "      <th>NVVar4</th>\n",
       "      <th>Claim_Amount</th>\n",
       "      <th>Stratifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21430</th>\n",
       "      <td>2959699</td>\n",
       "      <td>2063141</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.337415</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>4906216</td>\n",
       "      <td>3108586</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082855</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.429994</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23203</th>\n",
       "      <td>2668985</td>\n",
       "      <td>1916604</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.106594</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>8092873</td>\n",
       "      <td>5153020</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>N</td>\n",
       "      <td>2.054683</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>5.748932</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12305</th>\n",
       "      <td>8017601</td>\n",
       "      <td>5126989</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20116</th>\n",
       "      <td>4160788</td>\n",
       "      <td>2608193</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>8869889</td>\n",
       "      <td>5803719</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18735</th>\n",
       "      <td>4055236</td>\n",
       "      <td>2573687</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>1652478</td>\n",
       "      <td>939349</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>3706292</td>\n",
       "      <td>2448524</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>2710151</td>\n",
       "      <td>1930600</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>79.588900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17039</th>\n",
       "      <td>2077526</td>\n",
       "      <td>1240636</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.723858</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_ID  Household_ID  Vehicle  Calendar_Year  Model_Year Blind_Make  \\\n",
       "21430  2959699       2063141        1           2006        2004          ?   \n",
       "6606   4906216       3108586        1           2005        2002          ?   \n",
       "23203  2668985       1916604        1           2005        2003          ?   \n",
       "17886  8092873       5153020        1           2005        2003          ?   \n",
       "12305  8017601       5126989        1           2007        2002          ?   \n",
       "20116  4160788       2608193        1           2007        2004          ?   \n",
       "19932  8869889       5803719        2           2006        2003          ?   \n",
       "18735  4055236       2573687        1           2005        2002          ?   \n",
       "16665  1652478        939349        2           2007        2002          ?   \n",
       "20602  3706292       2448524        1           2005        2002          ?   \n",
       "15149  2710151       1930600        1           2007        2004          ?   \n",
       "17039  2077526       1240636        1           2006        2004          ?   \n",
       "\n",
       "      Blind_Model Blind_Submodel Cat1 Cat2  ...      Var6      Var7      Var8  \\\n",
       "21430           ?              ?    F    C  ...  0.498491 -0.972211  2.337415   \n",
       "6606            ?              ?    A    C  ... -0.082855 -0.677309  1.429994   \n",
       "23203           ?              ?    F    C  ...  0.059850 -0.972211  2.106594   \n",
       "17886           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "12305           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "20116           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "19932           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "18735           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "16665           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "20602           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "15149           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "17039           ?              ?    B    C  ... -0.211523 -0.800185  0.723858   \n",
       "\n",
       "      NVCat    NVVar1    NVVar2    NVVar3    NVVar4 Claim_Amount Stratifier  \n",
       "21430     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "6606      M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "23203     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "17886     N  2.054683 -0.266117 -0.272337 -0.251419     5.748932        1.0  \n",
       "12305     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20116     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "19932     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "18735     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "16665     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20602     N -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "15149     M -0.231530 -0.266117 -0.272337 -0.251419    79.588900        1.0  \n",
       "17039     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "\n",
       "[12 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[train_set['Blind_Model']=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_ID</th>\n",
       "      <th>Household_ID</th>\n",
       "      <th>Vehicle</th>\n",
       "      <th>Calendar_Year</th>\n",
       "      <th>Model_Year</th>\n",
       "      <th>Blind_Make</th>\n",
       "      <th>Blind_Model</th>\n",
       "      <th>Blind_Submodel</th>\n",
       "      <th>Cat1</th>\n",
       "      <th>Cat2</th>\n",
       "      <th>...</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>NVCat</th>\n",
       "      <th>NVVar1</th>\n",
       "      <th>NVVar2</th>\n",
       "      <th>NVVar3</th>\n",
       "      <th>NVVar4</th>\n",
       "      <th>Claim_Amount</th>\n",
       "      <th>Stratifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21430</th>\n",
       "      <td>2959699</td>\n",
       "      <td>2063141</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.337415</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6606</th>\n",
       "      <td>4906216</td>\n",
       "      <td>3108586</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082855</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.429994</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23203</th>\n",
       "      <td>2668985</td>\n",
       "      <td>1916604</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>-0.972211</td>\n",
       "      <td>2.106594</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>8092873</td>\n",
       "      <td>5153020</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>N</td>\n",
       "      <td>2.054683</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>5.748932</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12305</th>\n",
       "      <td>8017601</td>\n",
       "      <td>5126989</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20116</th>\n",
       "      <td>4160788</td>\n",
       "      <td>2608193</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19932</th>\n",
       "      <td>8869889</td>\n",
       "      <td>5803719</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>2003</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.613370</td>\n",
       "      <td>O</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18735</th>\n",
       "      <td>4055236</td>\n",
       "      <td>2573687</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16665</th>\n",
       "      <td>1652478</td>\n",
       "      <td>939349</td>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.598602</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20602</th>\n",
       "      <td>3706292</td>\n",
       "      <td>2448524</td>\n",
       "      <td>1</td>\n",
       "      <td>2005</td>\n",
       "      <td>2002</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188518</td>\n",
       "      <td>-0.677309</td>\n",
       "      <td>1.558532</td>\n",
       "      <td>N</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15149</th>\n",
       "      <td>2710151</td>\n",
       "      <td>1930600</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391659</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.494131</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>79.588900</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17039</th>\n",
       "      <td>2077526</td>\n",
       "      <td>1240636</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>2004</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.211523</td>\n",
       "      <td>-0.800185</td>\n",
       "      <td>0.723858</td>\n",
       "      <td>M</td>\n",
       "      <td>-0.231530</td>\n",
       "      <td>-0.266117</td>\n",
       "      <td>-0.272337</td>\n",
       "      <td>-0.251419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Row_ID  Household_ID  Vehicle  Calendar_Year  Model_Year Blind_Make  \\\n",
       "21430  2959699       2063141        1           2006        2004          ?   \n",
       "6606   4906216       3108586        1           2005        2002          ?   \n",
       "23203  2668985       1916604        1           2005        2003          ?   \n",
       "17886  8092873       5153020        1           2005        2003          ?   \n",
       "12305  8017601       5126989        1           2007        2002          ?   \n",
       "20116  4160788       2608193        1           2007        2004          ?   \n",
       "19932  8869889       5803719        2           2006        2003          ?   \n",
       "18735  4055236       2573687        1           2005        2002          ?   \n",
       "16665  1652478        939349        2           2007        2002          ?   \n",
       "20602  3706292       2448524        1           2005        2002          ?   \n",
       "15149  2710151       1930600        1           2007        2004          ?   \n",
       "17039  2077526       1240636        1           2006        2004          ?   \n",
       "\n",
       "      Blind_Model Blind_Submodel Cat1 Cat2  ...      Var6      Var7      Var8  \\\n",
       "21430           ?              ?    F    C  ...  0.498491 -0.972211  2.337415   \n",
       "6606            ?              ?    A    C  ... -0.082855 -0.677309  1.429994   \n",
       "23203           ?              ?    F    C  ...  0.059850 -0.972211  2.106594   \n",
       "17886           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "12305           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "20116           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "19932           ?              ?    B    C  ... -0.211523 -0.800185  0.613370   \n",
       "18735           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "16665           ?              ?    B    C  ... -0.211523 -0.800185  0.598602   \n",
       "20602           ?              ?    A    C  ...  0.188518 -0.677309  1.558532   \n",
       "15149           ?              ?    B    C  ... -0.391659 -0.800185  0.494131   \n",
       "17039           ?              ?    B    C  ... -0.211523 -0.800185  0.723858   \n",
       "\n",
       "      NVCat    NVVar1    NVVar2    NVVar3    NVVar4 Claim_Amount Stratifier  \n",
       "21430     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "6606      M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "23203     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "17886     N  2.054683 -0.266117 -0.272337 -0.251419     5.748932        1.0  \n",
       "12305     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20116     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "19932     O -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "18735     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "16665     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "20602     N -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "15149     M -0.231530 -0.266117 -0.272337 -0.251419    79.588900        1.0  \n",
       "17039     M -0.231530 -0.266117 -0.272337 -0.251419     0.000000        0.0  \n",
       "\n",
       "[12 rows x 36 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[train_set['Blind_Submodel']=='?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blind_Make, Blind_Model and Blind_Submodel all have the same missing values, we can drop these rows since they have multiple\n",
    "#missing values on our feature space. This will be dealt using the categorical_transformer function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1,\n",
       " 'B': 2,\n",
       " 'C': 3,\n",
       " 'D': 4,\n",
       " 'E': 5,\n",
       " 'F': 6,\n",
       " 'G': 7,\n",
       " 'H': 8,\n",
       " 'I': 9,\n",
       " 'J': 10,\n",
       " 'K': 11,\n",
       " 'L': 12,\n",
       " 'M': 13,\n",
       " 'N': 14,\n",
       " 'O': 15,\n",
       " 'P': 16,\n",
       " 'Q': 17,\n",
       " 'R': 18,\n",
       " 'S': 19,\n",
       " 'T': 20,\n",
       " 'U': 21,\n",
       " 'V': 22,\n",
       " 'W': 23,\n",
       " 'X': 24,\n",
       " 'Y': 25,\n",
       " 'Z': 26,\n",
       " 'AA': 27,\n",
       " 'AB': 28,\n",
       " 'AC': 29,\n",
       " 'AD': 30,\n",
       " 'AE': 31,\n",
       " 'AF': 32,\n",
       " 'AG': 33,\n",
       " 'AH': 34,\n",
       " 'AI': 35,\n",
       " 'AJ': 36,\n",
       " 'AK': 37,\n",
       " 'AL': 38,\n",
       " 'AM': 39,\n",
       " 'AN': 40,\n",
       " 'AO': 41,\n",
       " 'AP': 42,\n",
       " 'AQ': 43,\n",
       " 'AR': 44,\n",
       " 'AS': 45,\n",
       " 'AT': 46,\n",
       " 'AU': 47,\n",
       " 'AV': 48,\n",
       " 'AW': 49,\n",
       " 'AX': 50,\n",
       " 'AY': 51,\n",
       " 'AZ': 52,\n",
       " 'BA': 53,\n",
       " 'BB': 54,\n",
       " 'BC': 55,\n",
       " 'BD': 56,\n",
       " 'BE': 57,\n",
       " 'BF': 58,\n",
       " 'BG': 59,\n",
       " 'BH': 60,\n",
       " 'BI': 61,\n",
       " 'BJ': 62,\n",
       " 'BK': 63,\n",
       " 'BL': 64,\n",
       " 'BM': 65,\n",
       " 'BN': 66,\n",
       " 'BO': 67,\n",
       " 'BP': 68,\n",
       " 'BQ': 69,\n",
       " 'BR': 70,\n",
       " 'BS': 71,\n",
       " 'BT': 72,\n",
       " 'BU': 73,\n",
       " 'BV': 74,\n",
       " 'BW': 75,\n",
       " 'BX': 76,\n",
       " 'BY': 77,\n",
       " 'BZ': 78,\n",
       " 'CA': 79,\n",
       " 'CB': 80,\n",
       " 'CC': 81,\n",
       " 'CD': 82,\n",
       " 'CE': 83,\n",
       " 'CF': 84,\n",
       " 'CG': 85,\n",
       " 'CH': 86,\n",
       " 'CI': 87,\n",
       " 'CJ': 88,\n",
       " 'CK': 89,\n",
       " 'CL': 90,\n",
       " 'CM': 91,\n",
       " 'CN': 92,\n",
       " 'CO': 93,\n",
       " 'CP': 94,\n",
       " 'CQ': 95,\n",
       " 'CR': 96,\n",
       " 'CS': 97,\n",
       " 'CT': 98,\n",
       " 'CU': 99,\n",
       " 'CV': 100,\n",
       " 'CW': 101,\n",
       " 'CX': 102,\n",
       " 'CY': 103,\n",
       " 'CZ': 104,\n",
       " 'DA': 105,\n",
       " 'DB': 106,\n",
       " 'DC': 107,\n",
       " 'DD': 108,\n",
       " 'DE': 109,\n",
       " 'DF': 110,\n",
       " 'DG': 111,\n",
       " 'DH': 112,\n",
       " 'DI': 113,\n",
       " 'DJ': 114,\n",
       " 'DK': 115,\n",
       " 'DL': 116,\n",
       " 'DM': 117,\n",
       " 'DN': 118,\n",
       " 'DO': 119,\n",
       " 'DP': 120,\n",
       " 'DQ': 121,\n",
       " 'DR': 122,\n",
       " 'DS': 123,\n",
       " 'DT': 124,\n",
       " 'DU': 125,\n",
       " 'DV': 126,\n",
       " 'DW': 127,\n",
       " 'DX': 128,\n",
       " 'DY': 129,\n",
       " 'DZ': 130,\n",
       " 'EA': 131,\n",
       " 'EB': 132,\n",
       " 'EC': 133,\n",
       " 'ED': 134,\n",
       " 'EE': 135,\n",
       " 'EF': 136,\n",
       " 'EG': 137,\n",
       " 'EH': 138,\n",
       " 'EI': 139,\n",
       " 'EJ': 140,\n",
       " 'EK': 141,\n",
       " 'EL': 142,\n",
       " 'EM': 143,\n",
       " 'EN': 144,\n",
       " 'EO': 145,\n",
       " 'EP': 146,\n",
       " 'EQ': 147,\n",
       " 'ER': 148,\n",
       " 'ES': 149,\n",
       " 'ET': 150,\n",
       " 'EU': 151,\n",
       " 'EV': 152,\n",
       " 'EW': 153,\n",
       " 'EX': 154,\n",
       " 'EY': 155,\n",
       " 'EZ': 156,\n",
       " 'FA': 157,\n",
       " 'FB': 158,\n",
       " 'FC': 159,\n",
       " 'FD': 160,\n",
       " 'FE': 161,\n",
       " 'FF': 162,\n",
       " 'FG': 163,\n",
       " 'FH': 164,\n",
       " 'FI': 165,\n",
       " 'FJ': 166,\n",
       " 'FK': 167,\n",
       " 'FL': 168,\n",
       " 'FM': 169,\n",
       " 'FN': 170,\n",
       " 'FO': 171,\n",
       " 'FP': 172,\n",
       " 'FQ': 173,\n",
       " 'FR': 174,\n",
       " 'FS': 175,\n",
       " 'FT': 176,\n",
       " 'FU': 177,\n",
       " 'FV': 178,\n",
       " 'FW': 179,\n",
       " 'FX': 180,\n",
       " 'FY': 181,\n",
       " 'FZ': 182,\n",
       " 'GA': 183,\n",
       " 'GB': 184,\n",
       " 'GC': 185,\n",
       " 'GD': 186,\n",
       " 'GE': 187,\n",
       " 'GF': 188,\n",
       " 'GG': 189,\n",
       " 'GH': 190,\n",
       " 'GI': 191,\n",
       " 'GJ': 192,\n",
       " 'GK': 193,\n",
       " 'GL': 194,\n",
       " 'GM': 195,\n",
       " 'GN': 196,\n",
       " 'GO': 197,\n",
       " 'GP': 198,\n",
       " 'GQ': 199,\n",
       " 'GR': 200,\n",
       " 'GS': 201,\n",
       " 'GT': 202,\n",
       " 'GU': 203,\n",
       " 'GV': 204,\n",
       " 'GW': 205,\n",
       " 'GX': 206,\n",
       " 'GY': 207,\n",
       " 'GZ': 208,\n",
       " 'HA': 209,\n",
       " 'HB': 210,\n",
       " 'HC': 211,\n",
       " 'HD': 212,\n",
       " 'HE': 213,\n",
       " 'HF': 214,\n",
       " 'HG': 215,\n",
       " 'HH': 216,\n",
       " 'HI': 217,\n",
       " 'HJ': 218,\n",
       " 'HK': 219,\n",
       " 'HL': 220,\n",
       " 'HM': 221,\n",
       " 'HN': 222,\n",
       " 'HO': 223,\n",
       " 'HP': 224,\n",
       " 'HQ': 225,\n",
       " 'HR': 226,\n",
       " 'HS': 227,\n",
       " 'HT': 228,\n",
       " 'HU': 229,\n",
       " 'HV': 230,\n",
       " 'HW': 231,\n",
       " 'HX': 232,\n",
       " 'HY': 233,\n",
       " 'HZ': 234,\n",
       " 'IA': 235,\n",
       " 'IB': 236,\n",
       " 'IC': 237,\n",
       " 'ID': 238,\n",
       " 'IE': 239,\n",
       " 'IF': 240,\n",
       " 'IG': 241,\n",
       " 'IH': 242,\n",
       " 'II': 243,\n",
       " 'IJ': 244,\n",
       " 'IK': 245,\n",
       " 'IL': 246,\n",
       " 'IM': 247,\n",
       " 'IN': 248,\n",
       " 'IO': 249,\n",
       " 'IP': 250,\n",
       " 'IQ': 251,\n",
       " 'IR': 252,\n",
       " 'IS': 253,\n",
       " 'IT': 254,\n",
       " 'IU': 255,\n",
       " 'IV': 256,\n",
       " 'IW': 257,\n",
       " 'IX': 258,\n",
       " 'IY': 259,\n",
       " 'IZ': 260,\n",
       " 'JA': 261,\n",
       " 'JB': 262,\n",
       " 'JC': 263,\n",
       " 'JD': 264,\n",
       " 'JE': 265,\n",
       " 'JF': 266,\n",
       " 'JG': 267,\n",
       " 'JH': 268,\n",
       " 'JI': 269,\n",
       " 'JJ': 270,\n",
       " 'JK': 271,\n",
       " 'JL': 272,\n",
       " 'JM': 273,\n",
       " 'JN': 274,\n",
       " 'JO': 275,\n",
       " 'JP': 276,\n",
       " 'JQ': 277,\n",
       " 'JR': 278,\n",
       " 'JS': 279,\n",
       " 'JT': 280,\n",
       " 'JU': 281,\n",
       " 'JV': 282,\n",
       " 'JW': 283,\n",
       " 'JX': 284,\n",
       " 'JY': 285,\n",
       " 'JZ': 286,\n",
       " 'KA': 287,\n",
       " 'KB': 288,\n",
       " 'KC': 289,\n",
       " 'KD': 290,\n",
       " 'KE': 291,\n",
       " 'KF': 292,\n",
       " 'KG': 293,\n",
       " 'KH': 294,\n",
       " 'KI': 295,\n",
       " 'KJ': 296,\n",
       " 'KK': 297,\n",
       " 'KL': 298,\n",
       " 'KM': 299,\n",
       " 'KN': 300,\n",
       " 'KO': 301,\n",
       " 'KP': 302,\n",
       " 'KQ': 303,\n",
       " 'KR': 304,\n",
       " 'KS': 305,\n",
       " 'KT': 306,\n",
       " 'KU': 307,\n",
       " 'KV': 308,\n",
       " 'KW': 309,\n",
       " 'KX': 310,\n",
       " 'KY': 311,\n",
       " 'KZ': 312,\n",
       " 'LA': 313,\n",
       " 'LB': 314,\n",
       " 'LC': 315,\n",
       " 'LD': 316,\n",
       " 'LE': 317,\n",
       " 'LF': 318,\n",
       " 'LG': 319,\n",
       " 'LH': 320,\n",
       " 'LI': 321,\n",
       " 'LJ': 322,\n",
       " 'LK': 323,\n",
       " 'LL': 324,\n",
       " 'LM': 325,\n",
       " 'LN': 326,\n",
       " 'LO': 327,\n",
       " 'LP': 328,\n",
       " 'LQ': 329,\n",
       " 'LR': 330,\n",
       " 'LS': 331,\n",
       " 'LT': 332,\n",
       " 'LU': 333,\n",
       " 'LV': 334,\n",
       " 'LW': 335,\n",
       " 'LX': 336,\n",
       " 'LY': 337,\n",
       " 'LZ': 338,\n",
       " 'MA': 339,\n",
       " 'MB': 340,\n",
       " 'MC': 341,\n",
       " 'MD': 342,\n",
       " 'ME': 343,\n",
       " 'MF': 344,\n",
       " 'MG': 345,\n",
       " 'MH': 346,\n",
       " 'MI': 347,\n",
       " 'MJ': 348,\n",
       " 'MK': 349,\n",
       " 'ML': 350,\n",
       " 'MM': 351,\n",
       " 'MN': 352,\n",
       " 'MO': 353,\n",
       " 'MP': 354,\n",
       " 'MQ': 355,\n",
       " 'MR': 356,\n",
       " 'MS': 357,\n",
       " 'MT': 358,\n",
       " 'MU': 359,\n",
       " 'MV': 360,\n",
       " 'MW': 361,\n",
       " 'MX': 362,\n",
       " 'MY': 363,\n",
       " 'MZ': 364,\n",
       " 'NA': 365,\n",
       " 'NB': 366,\n",
       " 'NC': 367,\n",
       " 'ND': 368,\n",
       " 'NE': 369,\n",
       " 'NF': 370,\n",
       " 'NG': 371,\n",
       " 'NH': 372,\n",
       " 'NI': 373,\n",
       " 'NJ': 374,\n",
       " 'NK': 375,\n",
       " 'NL': 376,\n",
       " 'NM': 377,\n",
       " 'NN': 378,\n",
       " 'NO': 379,\n",
       " 'NP': 380,\n",
       " 'NQ': 381,\n",
       " 'NR': 382,\n",
       " 'NS': 383,\n",
       " 'NT': 384,\n",
       " 'NU': 385,\n",
       " 'NV': 386,\n",
       " 'NW': 387,\n",
       " 'NX': 388,\n",
       " 'NY': 389,\n",
       " 'NZ': 390,\n",
       " 'OA': 391,\n",
       " 'OB': 392,\n",
       " 'OC': 393,\n",
       " 'OD': 394,\n",
       " 'OE': 395,\n",
       " 'OF': 396,\n",
       " 'OG': 397,\n",
       " 'OH': 398,\n",
       " 'OI': 399,\n",
       " 'OJ': 400,\n",
       " 'OK': 401,\n",
       " 'OL': 402,\n",
       " 'OM': 403,\n",
       " 'ON': 404,\n",
       " 'OO': 405,\n",
       " 'OP': 406,\n",
       " 'OQ': 407,\n",
       " 'OR': 408,\n",
       " 'OS': 409,\n",
       " 'OT': 410,\n",
       " 'OU': 411,\n",
       " 'OV': 412,\n",
       " 'OW': 413,\n",
       " 'OX': 414,\n",
       " 'OY': 415,\n",
       " 'OZ': 416,\n",
       " 'PA': 417,\n",
       " 'PB': 418,\n",
       " 'PC': 419,\n",
       " 'PD': 420,\n",
       " 'PE': 421,\n",
       " 'PF': 422,\n",
       " 'PG': 423,\n",
       " 'PH': 424,\n",
       " 'PI': 425,\n",
       " 'PJ': 426,\n",
       " 'PK': 427,\n",
       " 'PL': 428,\n",
       " 'PM': 429,\n",
       " 'PN': 430,\n",
       " 'PO': 431,\n",
       " 'PP': 432,\n",
       " 'PQ': 433,\n",
       " 'PR': 434,\n",
       " 'PS': 435,\n",
       " 'PT': 436,\n",
       " 'PU': 437,\n",
       " 'PV': 438,\n",
       " 'PW': 439,\n",
       " 'PX': 440,\n",
       " 'PY': 441,\n",
       " 'PZ': 442,\n",
       " 'QA': 443,\n",
       " 'QB': 444,\n",
       " 'QC': 445,\n",
       " 'QD': 446,\n",
       " 'QE': 447,\n",
       " 'QF': 448,\n",
       " 'QG': 449,\n",
       " 'QH': 450,\n",
       " 'QI': 451,\n",
       " 'QJ': 452,\n",
       " 'QK': 453,\n",
       " 'QL': 454,\n",
       " 'QM': 455,\n",
       " 'QN': 456,\n",
       " 'QO': 457,\n",
       " 'QP': 458,\n",
       " 'QQ': 459,\n",
       " 'QR': 460,\n",
       " 'QS': 461,\n",
       " 'QT': 462,\n",
       " 'QU': 463,\n",
       " 'QV': 464,\n",
       " 'QW': 465,\n",
       " 'QX': 466,\n",
       " 'QY': 467,\n",
       " 'QZ': 468,\n",
       " 'RA': 469,\n",
       " 'RB': 470,\n",
       " 'RC': 471,\n",
       " 'RD': 472,\n",
       " 'RE': 473,\n",
       " 'RF': 474,\n",
       " 'RG': 475,\n",
       " 'RH': 476,\n",
       " 'RI': 477,\n",
       " 'RJ': 478,\n",
       " 'RK': 479,\n",
       " 'RL': 480,\n",
       " 'RM': 481,\n",
       " 'RN': 482,\n",
       " 'RO': 483,\n",
       " 'RP': 484,\n",
       " 'RQ': 485,\n",
       " 'RR': 486,\n",
       " 'RS': 487,\n",
       " 'RT': 488,\n",
       " 'RU': 489,\n",
       " 'RV': 490,\n",
       " 'RW': 491,\n",
       " 'RX': 492,\n",
       " 'RY': 493,\n",
       " 'RZ': 494,\n",
       " 'SA': 495,\n",
       " 'SB': 496,\n",
       " 'SC': 497,\n",
       " 'SD': 498,\n",
       " 'SE': 499,\n",
       " 'SF': 500,\n",
       " 'SG': 501,\n",
       " 'SH': 502,\n",
       " 'SI': 503,\n",
       " 'SJ': 504,\n",
       " 'SK': 505,\n",
       " 'SL': 506,\n",
       " 'SM': 507,\n",
       " 'SN': 508,\n",
       " 'SO': 509,\n",
       " 'SP': 510,\n",
       " 'SQ': 511,\n",
       " 'SR': 512,\n",
       " 'SS': 513,\n",
       " 'ST': 514,\n",
       " 'SU': 515,\n",
       " 'SV': 516,\n",
       " 'SW': 517,\n",
       " 'SX': 518,\n",
       " 'SY': 519,\n",
       " 'SZ': 520,\n",
       " 'TA': 521,\n",
       " 'TB': 522,\n",
       " 'TC': 523,\n",
       " 'TD': 524,\n",
       " 'TE': 525,\n",
       " 'TF': 526,\n",
       " 'TG': 527,\n",
       " 'TH': 528,\n",
       " 'TI': 529,\n",
       " 'TJ': 530,\n",
       " 'TK': 531,\n",
       " 'TL': 532,\n",
       " 'TM': 533,\n",
       " 'TN': 534,\n",
       " 'TO': 535,\n",
       " 'TP': 536,\n",
       " 'TQ': 537,\n",
       " 'TR': 538,\n",
       " 'TS': 539,\n",
       " 'TT': 540,\n",
       " 'TU': 541,\n",
       " 'TV': 542,\n",
       " 'TW': 543,\n",
       " 'TX': 544,\n",
       " 'TY': 545,\n",
       " 'TZ': 546,\n",
       " 'UA': 547,\n",
       " 'UB': 548,\n",
       " 'UC': 549,\n",
       " 'UD': 550,\n",
       " 'UE': 551,\n",
       " 'UF': 552,\n",
       " 'UG': 553,\n",
       " 'UH': 554,\n",
       " 'UI': 555,\n",
       " 'UJ': 556,\n",
       " 'UK': 557,\n",
       " 'UL': 558,\n",
       " 'UM': 559,\n",
       " 'UN': 560,\n",
       " 'UO': 561,\n",
       " 'UP': 562,\n",
       " 'UQ': 563,\n",
       " 'UR': 564,\n",
       " 'US': 565,\n",
       " 'UT': 566,\n",
       " 'UU': 567,\n",
       " 'UV': 568,\n",
       " 'UW': 569,\n",
       " 'UX': 570,\n",
       " 'UY': 571,\n",
       " 'UZ': 572,\n",
       " 'VA': 573,\n",
       " 'VB': 574,\n",
       " 'VC': 575,\n",
       " 'VD': 576,\n",
       " 'VE': 577,\n",
       " 'VF': 578,\n",
       " 'VG': 579,\n",
       " 'VH': 580,\n",
       " 'VI': 581,\n",
       " 'VJ': 582,\n",
       " 'VK': 583,\n",
       " 'VL': 584,\n",
       " 'VM': 585,\n",
       " 'VN': 586,\n",
       " 'VO': 587,\n",
       " 'VP': 588,\n",
       " 'VQ': 589,\n",
       " 'VR': 590,\n",
       " 'VS': 591,\n",
       " 'VT': 592,\n",
       " 'VU': 593,\n",
       " 'VV': 594,\n",
       " 'VW': 595,\n",
       " 'VX': 596,\n",
       " 'VY': 597,\n",
       " 'VZ': 598,\n",
       " 'WA': 599,\n",
       " 'WB': 600,\n",
       " 'WC': 601,\n",
       " 'WD': 602,\n",
       " 'WE': 603,\n",
       " 'WF': 604,\n",
       " 'WG': 605,\n",
       " 'WH': 606,\n",
       " 'WI': 607,\n",
       " 'WJ': 608,\n",
       " 'WK': 609,\n",
       " 'WL': 610,\n",
       " 'WM': 611,\n",
       " 'WN': 612,\n",
       " 'WO': 613,\n",
       " 'WP': 614,\n",
       " 'WQ': 615,\n",
       " 'WR': 616,\n",
       " 'WS': 617,\n",
       " 'WT': 618,\n",
       " 'WU': 619,\n",
       " 'WV': 620,\n",
       " 'WW': 621,\n",
       " 'WX': 622,\n",
       " 'WY': 623,\n",
       " 'WZ': 624,\n",
       " 'XA': 625,\n",
       " 'XB': 626,\n",
       " 'XC': 627,\n",
       " 'XD': 628,\n",
       " 'XE': 629,\n",
       " 'XF': 630,\n",
       " 'XG': 631,\n",
       " 'XH': 632,\n",
       " 'XI': 633,\n",
       " 'XJ': 634,\n",
       " 'XK': 635,\n",
       " 'XL': 636,\n",
       " 'XM': 637,\n",
       " 'XN': 638,\n",
       " 'XO': 639,\n",
       " 'XP': 640,\n",
       " 'XQ': 641,\n",
       " 'XR': 642,\n",
       " 'XS': 643,\n",
       " 'XT': 644,\n",
       " 'XU': 645,\n",
       " 'XV': 646,\n",
       " 'XW': 647,\n",
       " 'XX': 648,\n",
       " 'XY': 649,\n",
       " 'XZ': 650,\n",
       " 'YA': 651,\n",
       " 'YB': 652,\n",
       " 'YC': 653,\n",
       " 'YD': 654,\n",
       " 'YE': 655,\n",
       " 'YF': 656,\n",
       " 'YG': 657,\n",
       " 'YH': 658,\n",
       " 'YI': 659,\n",
       " 'YJ': 660,\n",
       " 'YK': 661,\n",
       " 'YL': 662,\n",
       " 'YM': 663,\n",
       " 'YN': 664,\n",
       " 'YO': 665,\n",
       " 'YP': 666,\n",
       " 'YQ': 667,\n",
       " 'YR': 668,\n",
       " 'YS': 669,\n",
       " 'YT': 670,\n",
       " 'YU': 671,\n",
       " 'YV': 672,\n",
       " 'YW': 673,\n",
       " 'YX': 674,\n",
       " 'YY': 675,\n",
       " 'YZ': 676,\n",
       " 'ZA': 677,\n",
       " 'ZB': 678,\n",
       " 'ZC': 679,\n",
       " 'ZD': 680,\n",
       " 'ZE': 681,\n",
       " 'ZF': 682,\n",
       " 'ZG': 683,\n",
       " 'ZH': 684,\n",
       " 'ZI': 685,\n",
       " 'ZJ': 686,\n",
       " 'ZK': 687,\n",
       " 'ZL': 688,\n",
       " 'ZM': 689,\n",
       " 'ZN': 690,\n",
       " 'ZO': 691,\n",
       " 'ZP': 692,\n",
       " 'ZQ': 693,\n",
       " 'ZR': 694,\n",
       " 'ZS': 695,\n",
       " 'ZT': 696,\n",
       " 'ZU': 697,\n",
       " 'ZV': 698,\n",
       " 'ZW': 699,\n",
       " 'ZX': 700,\n",
       " 'ZY': 701,\n",
       " 'ZZ': 702}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Blind Make contains A,…,Z,AA,…,AZ,BA,…,BZ,… as per the data_dictionary file \n",
    "#Creating dictionary to convert Blind Make to Integer (LabelEncoding)\n",
    "\n",
    "alpha=list(string.ascii_uppercase)\n",
    "#Creating a list of single letter's (A,...,Z)\n",
    "ordinal_vals=alpha.copy()\n",
    "for letter_1 in alpha:\n",
    "    for letter_2 in alpha:\n",
    "        #Appending all possible 2 letter combinations of Blind Make values (AZ,BA,…,BZ,… )\n",
    "        ordinal_vals.append(letter_1+letter_2)\n",
    "        \n",
    "#Dictionary mapping Blind Make to integer values        \n",
    "ordinal_vals=dict(zip(ordinal_vals,list(range(1,len(ordinal_vals)+1))))\n",
    "\n",
    "ordinal_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to convert a string integer to integer\n",
    "#In case of non string integers, it returns -1\n",
    "def tryconvert(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a custom categorical transformer function to transform Blind_Make,Blind_Model,Blind_Submodel and OrdCat to numerical attributes\n",
    "#Preliminary transformation\n",
    "def categorical_transformer(dataset,ordinal_vals,ord_mode, missing_mode='train'):\n",
    "    \n",
    "    dataset= dataset.copy()\n",
    "    \n",
    "    if missing_mode=='train':\n",
    "        #Drop rows that have missing values in Blind_Make,Blind_Model and Blind_Submodel combined, since it has a lot of missing values\n",
    "        #We create copies to stop working with the dataframe view, which might cause assignment issues while passing them as arguments into functions\n",
    "        dataset= dataset[(dataset['Blind_Make']+dataset['Blind_Model']+dataset['Blind_Submodel'])!='???']\n",
    "        \n",
    "        #Map Blind_Make to integer values using the ordinal_vals dictionary\n",
    "        dataset.loc[:,'Blind_Make']=dataset['Blind_Make'].map(ordinal_vals)\n",
    "        #Blind_Make,Blind_Model and Blind_Submodel are related to each other, so we extract their unique portions/values and reassign them\n",
    "        dataset.loc[:,'Blind_Model'] =dataset['Blind_Model'].apply(lambda model : tryconvert(model.split('.')[1]))\n",
    "        dataset.loc[:,'Blind_Submodel']=dataset['Blind_Submodel'].apply(lambda submodel : tryconvert(submodel.split('.')[2]))\n",
    "        \n",
    "    else:\n",
    "\n",
    "        #Map Blind_Make to integer values using the ordinal_vals dictionary\n",
    "        dataset.loc[dataset['Blind_Make']!='?','Blind_Make']=dataset.loc[dataset['Blind_Make']!='?','Blind_Make'].map(ordinal_vals)\n",
    "        #Blind_Make,Blind_Model and Blind_Submodel are related to each other, so we extract their unique portions/values and reassign them\n",
    "        \n",
    "        #print (dataset.loc[dataset['Blind_Model']!='?','Blind_Model'])\n",
    "        \n",
    "        dataset.loc[dataset['Blind_Model']!='?','Blind_Model']= dataset.loc[dataset['Blind_Model']!='?','Blind_Model'].apply(lambda model : tryconvert(model.split('.')[1]))\n",
    "        dataset.loc[dataset['Blind_Submodel']!='?','Blind_Submodel']= dataset.loc[dataset['Blind_Submodel']!='?','Blind_Submodel'].apply(lambda submodel : tryconvert(submodel.split('.')[2]))\n",
    "         \n",
    "        dataset.loc[dataset['Blind_Make']=='?','Blind_Make']=np.NAN\n",
    "        dataset.loc[dataset['Blind_Model']=='?','Blind_Model']=np.NAN\n",
    "        dataset.loc[dataset['Blind_Submodel']=='?','Blind_Submodel']=np.NAN\n",
    "    \n",
    "    #ord_mode contains Ord_Cat mode from training set\n",
    "    #OrdCat is an ordinal category, we replace the missing values with ord_mode (mode of OrdCat), to avoid losing information \n",
    "    #ord_mode is used since we do not have a custom transform option (on validation and test set)\n",
    "    if '?' in dataset['OrdCat'].value_counts():\n",
    "        dataset.loc[dataset['OrdCat']=='?',['OrdCat']]=tryconvert(ord_mode)\n",
    "        \n",
    "    #Convert string integer to integer using fn tryconvert since it is an ordinal category   \n",
    "    dataset.loc[:,'OrdCat']=dataset['OrdCat'].apply(lambda val : tryconvert (val))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up feature space\n",
    "\n",
    "#We need to use atleast 5 Categorical features\n",
    "#Categorical features used are 'Blind_Make','Blind_Model','Blind_Submodel','OrdCat' and 'NVCat'\n",
    "\n",
    "ordinal_features=['Blind_Make','Blind_Model','Blind_Submodel','OrdCat']\n",
    "\n",
    "cat_features=['NVCat']\n",
    "\n",
    "#All numerical features except Row_ID are used\n",
    "numerical_features=['Household_ID','Vehicle', 'Calendar_Year', 'Model_Year', 'Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6',\n",
    " 'Var7', 'Var8', 'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4']\n",
    "\n",
    "#Feature Names\n",
    "feature_names=ordinal_features+cat_features+numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Transformation\n",
    "full_transform = ColumnTransformer([\n",
    "    \n",
    "    (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat_features), #Encodes Categorical Data\n",
    "    (\"num_imputer\", SimpleImputer(), numerical_features+ordinal_features) #Performs imputation on selected features\n",
    "    \n",
    "], remainder='passthrough',sparse_threshold=0)\n",
    "\n",
    "#(\"num_scaler\", StandardScaler(), numerical_features+ordinal_features) #Performs standardiation on selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminary transformation for train and validation sets\n",
    "#train_set['OrdCat'].mode()[0], is the mode of 'OrdCat', which is passed as ord_mode to set missing values\n",
    "\n",
    "train_set=categorical_transformer(train_set,ordinal_vals,train_set['OrdCat'].mode()[0])\n",
    "validation_set=categorical_transformer(validation_set,ordinal_vals,train_set['OrdCat'].mode()[0],missing_mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seting X,y for new train set\n",
    "X_train=train_set[feature_names]\n",
    "y_train=train_set.Claim_Amount\n",
    "\n",
    "#Seting X,y for new validation set\n",
    "X_validation=validation_set[feature_names]\n",
    "y_validation=validation_set.Claim_Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit transform in the train set using column transformer\n",
    "transformed_X_train=full_transform.fit_transform(X_train)\n",
    "#transform in the validation set using column transformer\n",
    "transformed_X_validation=full_transform.transform(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing under sampling on transformed_X_train,y_train to balance 0 and non 0 claims\n",
    "#Undersampling is performed using ClusterCentroids imblearn.under_sampling\n",
    "\n",
    "#Random Undersampler was not chosen due to the varying prediction results \n",
    "\n",
    "#Combine transformed_X_train with y_train to undersample them\n",
    "#We are using numpy.hstack since the transformed variables are numpy and dont have column names\n",
    "#y_train.to_numpy().reshape(-1,1) converts y_train to a numpy array of nx1 dimensions in order to stack them\n",
    "\n",
    "trans_X_y=np.hstack((transformed_X_train,y_train.to_numpy().reshape(-1,1)))\n",
    "\n",
    "#Initialising ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "\n",
    "#X_y_resampled stores the undersampled combination of transformed_X_train and y_train\n",
    "#y_stratifier contains undersampled 0 and non 0 claim values (represented as 0 & 1)\n",
    "X_y_resampled,y_stratifier = cc.fit_resample(trans_X_y, train_set.Stratifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassigning undersampled transformed_X_train and y_train\n",
    "transformed_X_train=X_y_resampled[:,:-1]\n",
    "y_train=X_y_resampled[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking up train and validation sets for using predefined split\n",
    "whole_X_train = np.vstack((transformed_X_train , transformed_X_validation))\n",
    "whole_y_train = np.hstack((y_train, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting predefined split for regression\n",
    "test_fold = np.zeros((np.shape(whole_X_train)[0], 1))\n",
    "test_fold[0:np.shape(transformed_X_train)[0]]= -1\n",
    "ps = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train different regressor models on the dataset and find out which one produces the best results\n",
    "\n",
    "#A regression model and a tandem model (combining classifer and regressor) will be compared to identify the best model \n",
    "\n",
    "#The models below were tuned using GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dict to store regressor scores\n",
    "regressor_scores={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Linear Regression Model on predefined validation set: 227.03544930801266\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Linear Regression model using grid search\n",
    "#Only tried tuning normalize and fit_intercept since the other parameters dont influence test_score\n",
    "\n",
    "param_grid={'normalize':[True,False],'fit_intercept':[True,False]}\n",
    "linear_regression=GridSearchCV(LinearRegression(),param_grid=param_grid,cv=ps,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "linear_regression.fit(whole_X_train,whole_y_train)\n",
    "print (f'Best RMSE value for the Linear Regression Model on predefined validation set: {-1*linear_regression.best_score_}')\n",
    "#pd.DataFrame(linear_regression.cv_results_)\n",
    "\n",
    "regressor_scores[('Linear',linear_regression.best_estimator_)]=-1*linear_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Ridge Regression Model on predefined validation set: 225.9305329146759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.02287e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.02287e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.02287e-18): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.0218e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.0218e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.0218e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.03183e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.03183e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n",
      "C:\\Users\\tevin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=3.03183e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Ridge Regression model using grid search\n",
    "\n",
    "param_grid={'normalize':[True,False],'alpha':[0.1, 1.0, 1.5],'max_iter':[10,100,1000]}\n",
    "ridge_regression=GridSearchCV(Ridge(),param_grid=param_grid,cv=ps,scoring='neg_root_mean_squared_error')\n",
    "\n",
    "ridge_regression.fit(whole_X_train,whole_y_train)\n",
    "print (f'Best RMSE value for the Ridge Regression Model on predefined validation set: {-1*ridge_regression.best_score_}')\n",
    "#pd.DataFrame(ridge_regression.cv_results_)\n",
    "regressor_scores[('Ridge',ridge_regression.best_estimator_)]=-1*ridge_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Random Forest Regression Model on predefined validation set: 242.68373323894858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Random Forest Regression model using grid search\n",
    "\n",
    "param_grid={'n_estimators':[50, 100, 200],'max_samples':[500, 1000, 2000],'max_depth':[5, 10, 15],'min_samples_split':[2,3,4]}\n",
    "random_forest_regression=GridSearchCV(RandomForestRegressor(random_state=342),param_grid=param_grid,cv=ps,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "random_forest_regression.fit(whole_X_train,whole_y_train)\n",
    "print (f'Best RMSE value for the Random Forest Regression Model on predefined validation set: {-1*random_forest_regression.best_score_}')\n",
    "#pd.DataFrame(random_forest_regression.cv_results_)\n",
    "regressor_scores[('Random Forest',random_forest_regression.best_estimator_)]=-1*random_forest_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Gradient Boost Regression Model on predefined validation set: 222.4267487767257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Train Gradient Tree Boosting for Regression model using grid search\n",
    "param_grid={'n_estimators':[50, 100, 200],'loss':['ls', 'lad', 'huber'],'learning_rate':[0.1, 0.2, 0.3]}\n",
    "gradient_boost_regression=GridSearchCV(GradientBoostingRegressor(),param_grid=param_grid,cv=ps,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "gradient_boost_regression.fit(whole_X_train,whole_y_train)\n",
    "print (f'Best RMSE value for the Gradient Boost Regression Model on predefined validation set: {-1*gradient_boost_regression.best_score_}')\n",
    "#pd.DataFrame(gradient_boost_regression.cv_results_)\n",
    "regressor_scores[('Gradient Boosting',gradient_boost_regression.best_estimator_)]=-1*gradient_boost_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to find the best regressor model for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Linear', LinearRegression(normalize=True)): 227.03544930801266,\n",
       " ('Ridge', Ridge(alpha=1.5, max_iter=10, normalize=True)): 225.9305329146759,\n",
       " ('Random Forest',\n",
       "  RandomForestRegressor(max_depth=5, max_samples=2000, min_samples_split=4,\n",
       "                        random_state=342)): 242.68373323894858,\n",
       " ('Gradient Boosting',\n",
       "  GradientBoostingRegressor(loss='huber', n_estimators=50)): 222.4267487767257}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Regressor Model\n",
    "best_regressor= min(regressor_scores, key=regressor_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor with the lowest RMSE is Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "print (f\"Regressor with the lowest RMSE is {best_regressor[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tandem Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Tandem_Model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train different classifier models to classify 0 and non zero claims (for Tandem model) on the dataset and find out\n",
    "#which one produces the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning training and validation set for tandem model classifier (0 or non Zero) with the help of column 'Stratifier'\n",
    "#We will use f1 score as a performance measure which is a combination of precision and recall for the classification model\n",
    "\n",
    "tand_classifier_X_train=transformed_X_train #Same transformed X_train set\n",
    "tand_classifier_y_train=y_stratifier #Uses the undersampled 0 and non 0 claims from above (represented as 0 and 1)\n",
    "\n",
    "tand_classifier_X_validation=transformed_X_validation #Same transformed X_validation set\n",
    "tand_classifier_y_validation=validation_set.Stratifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking up train and validation sets for classification based on previous predefined split (since they use same features and are of same size)\n",
    "whole_tand_class_X_train = np.vstack((tand_classifier_X_train , tand_classifier_X_validation))\n",
    "whole_tand_class_y_train = np.hstack((tand_classifier_y_train, tand_classifier_y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to store tandem classifer scores\n",
    "tand_classifier_scores={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for the Random Forest Classifier Model on predefined validation set: 0.45869134170522136\n"
     ]
    }
   ],
   "source": [
    "#Training Random Forest Classifier for binary classification\n",
    "\n",
    "param_grid={'n_estimators':[50, 100, 200],'criterion':['gini', 'entropy'],'max_depth':[5, 10, 15],'bootstrap':[True,False]}\n",
    "tand_rf_classifier=GridSearchCV(RandomForestClassifier(random_state=342),param_grid=param_grid,cv=ps,scoring='f1',n_jobs=-1)\n",
    "\n",
    "tand_rf_classifier.fit(whole_tand_class_X_train,whole_tand_class_y_train)\n",
    "print (f'Best F1 score for the Random Forest Classifier Model on predefined validation set: {tand_rf_classifier.best_score_}')\n",
    "#pd.DataFrame(tand_rf_classifier.cv_results_)\n",
    "tand_classifier_scores[('Random Forest',tand_rf_classifier.best_estimator_)]= tand_rf_classifier.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score for the Gradient Boost Classifier Model on predefined validation set: 0.4650862068965518\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training Gradient Boosting Classifier for binary classification\n",
    "\n",
    "param_grid={'n_estimators':[50,100, 200],'loss':['deviance', 'exponential'],'learning_rate':[0.1, 0.2, 0.3]}\n",
    "tand_gb_classifier=GridSearchCV(GradientBoostingClassifier(),param_grid=param_grid,cv=ps,scoring='f1',n_jobs=-1)\n",
    "\n",
    "tand_gb_classifier.fit(whole_tand_class_X_train,whole_tand_class_y_train)\n",
    "print (f'Best F1 score for the Gradient Boost Classifier Model on predefined validation set: {tand_gb_classifier.best_score_}')\n",
    "#pd.DataFrame(tand_gb_classifier.cv_results_)\n",
    "tand_classifier_scores[('Gradient Boosting',tand_gb_classifier.best_estimator_)]= tand_gb_classifier.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train different regressor models to predict the values for the non zero claims (for Tandem model) and find out\n",
    "#which one produces the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning training and validation set for tandem model regression with the help of column 'Startifier', filtering non Zero Claims\n",
    "\n",
    "tand_regression_X_train=transformed_X_train[y_stratifier==1] #Taking non 0 claim rows from transformed_X_train\n",
    "tand_regression_y_train=y_train[y_stratifier==1] #Taking non 0 claims from y_train\n",
    "\n",
    "tand_regression_X_validation=transformed_X_validation[validation_set['Stratifier']==1]  #Taking non 0 claim rows from transformed_X_validation\n",
    "tand_regression_y_validation=validation_set.Claim_Amount[validation_set['Stratifier']==1] #Taking non 0 claims from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stacking up train and validation sets for regression\n",
    "whole_tand_reg_X_train = np.vstack((tand_regression_X_train , tand_regression_X_validation))\n",
    "whole_tand_reg_y_train = np.hstack((tand_regression_y_train, tand_regression_y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new Predefined Split w.r.t filtered train and validation set\n",
    "test_fold = np.zeros((np.shape(whole_tand_reg_X_train)[0], 1))\n",
    "test_fold[0:np.shape(tand_regression_X_train)[0]]= -1\n",
    "ps_subset = PredefinedSplit(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict to store tandem regressor scores\n",
    "tand_regressor_scores={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Linear Regression Model on predefined validation set: 377.6660389287675\n"
     ]
    }
   ],
   "source": [
    "#Train Linear Regression model for nonzero claims\n",
    "\n",
    "param_grid={'normalize':[True,False],'fit_intercept':[True,False]}\n",
    "tand_lin_regression=GridSearchCV(LinearRegression(),param_grid=param_grid,cv=ps_subset,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "tand_lin_regression.fit(whole_tand_reg_X_train,whole_tand_reg_y_train)\n",
    "print (f'Best RMSE value for the Linear Regression Model on predefined validation set: {-1*tand_lin_regression.best_score_}')\n",
    "#pd.DataFrame(tand_lin_regression.cv_results_)\n",
    "tand_regressor_scores[('Linear',tand_lin_regression.best_estimator_)]= -1*tand_lin_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Ridge Regression Model on predefined validation set: 377.06548890624464\n"
     ]
    }
   ],
   "source": [
    "#Train Ridge Regression model for nonzero claims\n",
    "\n",
    "param_grid={'normalize':[True,False],'alpha':[0.1, 1.0, 1.5],'max_iter':[10,100,1000]}\n",
    "tand_ridge_regression=GridSearchCV(Ridge(),param_grid=param_grid,cv=ps_subset,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "tand_ridge_regression.fit(whole_tand_reg_X_train,whole_tand_reg_y_train)\n",
    "print (f'Best RMSE value for the Ridge Regression Model on predefined validation set: {-1*tand_ridge_regression.best_score_}')\n",
    "#pd.DataFrame(tand_ridge_regression.cv_results_)\n",
    "\n",
    "tand_regressor_scores[('Ridge',tand_ridge_regression.best_estimator_)]= -1*tand_ridge_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Random Forest Regression Model on predefined validation set: 382.3174357405198\n"
     ]
    }
   ],
   "source": [
    "#Train Random Forest Regression model for nonzero claims\n",
    "\n",
    "param_grid={'n_estimators':[50, 100, 200],'max_samples':[500, 1000, 2000],'max_depth':[5, 10, 15]}\n",
    "tand_rf_regression=GridSearchCV(RandomForestRegressor(random_state=48),param_grid=param_grid,cv=ps_subset,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "tand_rf_regression.fit(whole_tand_reg_X_train,whole_tand_reg_y_train)\n",
    "print (f'Best RMSE value for the Random Forest Regression Model on predefined validation set: {-1*tand_rf_regression.best_score_}')\n",
    "#pd.DataFrame(tand_rf_regression.cv_results_)\n",
    "\n",
    "tand_regressor_scores[('Random Forest',tand_rf_regression.best_estimator_)]= -1*tand_rf_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE value for the Gradient Boost Regression Model on predefined validation set: 382.03317818464274\n"
     ]
    }
   ],
   "source": [
    "#Train Graadient Boost Regression model for nonzero claims\n",
    "\n",
    "param_grid={'n_estimators':[50, 100, 200],'loss':['ls', 'lad', 'huber'],'learning_rate':[0.1, 0.2, 0.3]}\n",
    "tand_gb_regression=GridSearchCV(GradientBoostingRegressor(random_state=42),param_grid=param_grid,cv=ps_subset,scoring='neg_root_mean_squared_error',n_jobs=-1)\n",
    "\n",
    "tand_gb_regression.fit(whole_tand_reg_X_train,whole_tand_reg_y_train)\n",
    "print (f'Best RMSE value for the Gradient Boost Regression Model on predefined validation set: {-1*tand_gb_regression.best_score_}')\n",
    "#pd.DataFrame(tand_gb_regression.cv_results_)\n",
    "tand_regressor_scores[('Gradient Boosting',tand_gb_regression.best_estimator_)]= -1*tand_gb_regression.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to find the best classifier and the best regressor for the tandem model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Random Forest',\n",
       "  RandomForestClassifier(max_depth=5, n_estimators=50, random_state=342)): 0.45869134170522136,\n",
       " ('Gradient Boosting',\n",
       "  GradientBoostingClassifier(learning_rate=0.3, loss='exponential')): 0.4650862068965518}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tand_classifier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Tandem Classifier\n",
    "best_tand_class= max(tand_classifier_scores, key=tand_classifier_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with the highest score for tandem model is Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "print (f\"Classifier with the highest score for tandem model is {best_tand_class[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('Linear', LinearRegression(normalize=True)): 377.6660389287675,\n",
       " ('Ridge', Ridge(max_iter=10, normalize=True)): 377.06548890624464,\n",
       " ('Random Forest',\n",
       "  RandomForestRegressor(max_depth=5, max_samples=1000, n_estimators=200,\n",
       "                        random_state=48)): 382.3174357405198,\n",
       " ('Gradient Boosting',\n",
       "  GradientBoostingRegressor(loss='huber', n_estimators=50, random_state=42)): 382.03317818464274}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tand_regressor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Tandem Regressor\n",
    "best_tand_reg= min(tand_regressor_scores, key=tand_regressor_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor with the lowest RMSE for tandem model is Ridge\n"
     ]
    }
   ],
   "source": [
    "print (f\"Regressor with the lowest RMSE for tandem model is {best_tand_reg[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We chose Gradient Boosting model for part A and Ridge model for part B of the tandem model since they showed the most promising results\n"
     ]
    }
   ],
   "source": [
    "print (f\"We chose {best_tand_class[0]} model for part A and {best_tand_reg[0]} model\\\n",
    " for part B of the tandem model since they showed the most promising results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value for Tandem Model (trained without validation set) on test set is 249.45892217765243\n"
     ]
    }
   ],
   "source": [
    "#Tandem Model prediction on Validation set and computing RMSE on Validation set\n",
    "\n",
    "#Classify validation set using random forest classifier into 0 and non 0\n",
    "bin_class_pred=best_tand_class[1].predict(tand_classifier_X_validation)\n",
    "\n",
    "if bin_class_pred.sum()!=0:\n",
    "    #If Classifier predicts non 0 claims\n",
    "    \n",
    "    #Predict the regerssion value on non 0 validation set using linear regression model\n",
    "    reg_pred=best_tand_reg[1].predict(tand_classifier_X_validation[bin_class_pred==1])\n",
    "\n",
    "    #Create a new np array to combine both classification and regression predictions\n",
    "    y_val_predicted=np.zeros_like(y_validation)\n",
    "    y_val_predicted[bin_class_pred==1]=reg_pred\n",
    "    \n",
    "else:\n",
    "    #If Classifier predicts all 0 claims\n",
    "    \n",
    "    y_val_predicted=bin_class_pred\n",
    "\n",
    "#Calculate RMSE for Tandem Model\n",
    "tandem_rmse=sqrt(mean_squared_error(y_val_predicted, y_validation))\n",
    "print (f'RMSE value for Tandem Model (trained without validation set) on test set is {tandem_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retraining the best model after comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retraining the best model using combined train and validation set\n",
    "\n",
    "#Preliminary tranformation on training data\n",
    "final_train=categorical_transformer(original_train_set,ordinal_vals,original_train_set['OrdCat'].mode()[0])\n",
    "\n",
    "#Setting X for the binary classifier, target variable is the new 0,1 column ('Stratifier')\n",
    "final_X_clftrain=final_train[feature_names]\n",
    "\n",
    "#Applying column transformation (refitting) on training data\n",
    "tfd_final_X_clftrain= full_transform.fit_transform(final_X_clftrain)\n",
    "\n",
    "#Combine transformed_X_train with y_train to undersample them\n",
    "trans_X_y_train= np.hstack((tfd_final_X_clftrain,final_train.Claim_Amount.to_numpy().reshape(-1,1)))\n",
    "\n",
    "#Performing under sampling on transformed_final_X_clf_train and final_y_clftrain to balance 0 and non 0 claims\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "\n",
    "#final_X_y_resampled stores the resampled dataset\n",
    "final_X_y_resampled,final_y_stratifier = cc.fit_resample(trans_X_y_train, final_train.Stratifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reassigning undersampled final_X_clftrain and final_y_clftrain (Classification)\n",
    "final_X_clftrain=final_X_y_resampled[:,:-1]\n",
    "final_y_clftrain=final_y_stratifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting X for the regression problem by filtering on Non Zero Claim from the above transformed and undersampled X (final_X_clftrain) \n",
    "final_X_regtrain=final_X_clftrain[final_y_stratifier==1]\n",
    "\n",
    "#Setting y for the regression problem by filtering on Non Zero Claim \n",
    "final_y_regtrain=final_X_y_resampled[:,-1] #Obtaining Claims from the undersampled final_X_y_resampled\n",
    "final_y_regtrain=final_y_regtrain[final_y_stratifier==1] #Filtering on Non Zero Claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting model produces the best results\n"
     ]
    }
   ],
   "source": [
    "#Setting flag value to 1 (default: Tandem model) to find the best model\n",
    "best_model_flag=1\n",
    "\n",
    "#Compare tandem model and stand alone regressor performances and choose the best performing model\n",
    "if tandem_rmse<regressor_scores[best_regressor]:\n",
    "    #Tandem model performs better\n",
    "    \n",
    "    #Retrain best Classifier and Regression models of the tandem model\n",
    "    \n",
    "    #Retraining Classifier\n",
    "    tdf_gb_classifier= best_tand_class[1]\n",
    "    tdf_gb_classifier.fit(final_X_clftrain,final_y_clftrain)\n",
    "\n",
    "    #Retraining Regression Model\n",
    "    tdf_ridge_regression= best_tand_reg[1]\n",
    "    tdf_ridge_regression.fit(final_X_regtrain,final_y_regtrain)\n",
    "    \n",
    "    best_model= f'Tandem (Classifier: {best_tand_class[0]} | Regressor: {best_tand_reg[0]})'\n",
    "    print (f'Tandem model produces the best results with {best_tand_class[0]} Classifer and {best_tand_reg[0]} Regressor')\n",
    "        \n",
    "else:\n",
    "    #Standalone regressor performs better\n",
    "    \n",
    "    #Retraining best Regressor model on original train set\n",
    "    fgb_regression= best_regressor[1]\n",
    "    fgb_regression.fit(final_X_clftrain,final_X_y_resampled[:,-1].copy())\n",
    "    \n",
    "    best_model= f'{best_regressor[0]}'\n",
    "    print (f'{best_regressor[0]} model produces the best results')\n",
    "    \n",
    "    #Setting flag value to 2 (Standalone regressor Model)\n",
    "    best_model_flag=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function tandem_predictor() to predict claim amount based on the best tandem model\n",
    "def tandem_predictor(Xtest):\n",
    "\n",
    "    #Preliminary Transformation on test data\n",
    "    final_test=categorical_transformer(Xtest,ordinal_vals,original_train_set['OrdCat'].mode()[0],missing_mode='test')\n",
    "\n",
    "    #Splitting final test set into features (X) and target (y)\n",
    "    final_X_test=final_test[feature_names].copy()\n",
    "\n",
    "    #Column Transformer\n",
    "    final_X_test=full_transform.transform(final_X_test)\n",
    "\n",
    "    #Classify validation set using random forest classifier into 0 and non 0\n",
    "    bin_class_pred=tdf_gb_classifier.predict(final_X_test)\n",
    "\n",
    "    if bin_class_pred.sum()!=0:\n",
    "\n",
    "        #Predict the regerssion value on non 0 validation set using linear regression model\n",
    "        reg_pred=tdf_ridge_regression.predict(final_X_test[bin_class_pred==1])\n",
    "        #Create a new np array to combine both classification and regression predictions\n",
    "        y_val_predicted=np.zeros(final_X_test.shape[0])\n",
    "        y_val_predicted[bin_class_pred==1]=reg_pred\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Nothing to predict for regressor, all are 0 claims\n",
    "        y_val_predicted=bin_class_pred\n",
    "\n",
    "    return y_val_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function regressor_predictor() to predict claim amount based on the best regression model\n",
    "def regressor_predictor(Xtest):\n",
    "\n",
    "    #Preliminary Transformation on test data\n",
    "    final_test=categorical_transformer(Xtest,ordinal_vals,original_train_set['OrdCat'].mode()[0],missing_mode='test')\n",
    "\n",
    "    #Extracting features (X) from final test set\n",
    "    final_X_test=final_test[feature_names]\n",
    "\n",
    "    #Column Transformer\n",
    "    final_X_test=full_transform.transform(final_X_test)\n",
    "\n",
    "    #Predicting Claim using Gradient Boosting Regressor\n",
    "    y_val_predicted=fgb_regression.predict(final_X_test)\n",
    "\n",
    "    return y_val_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting my_insurance_claim_predictor to best model\n",
    "if best_model_flag==1:\n",
    "    #Tandem Model\n",
    "    my_insurance_claim_predictor= tandem_predictor\n",
    "else:\n",
    "    #Standalone Regressor Model\n",
    "    my_insurance_claim_predictor= regressor_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE value for Gradient Boosting Model (trained with validation set) on test set is 205.23387642691048\n"
     ]
    }
   ],
   "source": [
    "#Obtain model predictions on test set\n",
    "model_preds= my_insurance_claim_predictor(original_test_set)\n",
    "\n",
    "#Calculate RMSE on Test set for Tandem Model\n",
    "model_rmse=sqrt(mean_squared_error(model_preds, original_test_set.Claim_Amount.values))\n",
    "print (f'RMSE value for {best_model} Model (trained with validation set) on test set is {model_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting facts\n",
    "\n",
    "1) Choosing and Encoding Categorical Features\n",
    "\n",
    "The categorical features that have large number of unique values and low number of missing values will have more information in them. Using OneHotEncoder helps preserve categorical data at the cost of increasing feature space. The categorical features Blind Make, Model and Submodel have many unique values and if we use one hot encoding to convert these features, we will end up increasing our features space to the range of 2000s. These 3 features (Blind Make, Model & Submodel) are interlinked to each other. We could use this information to convert them to numerical data by removing the redundant parts of the value. Blind Submodel consists of Make, Model and Submodel, so we only keep whatever is unique to those respective columns. In this way, we could convert Blind Model and Submodel to numerical values (since they consist of numerical data). Blind make can be ordinally encoded based on the information we obtain from the data dictionary file. We create a list of possible values for blind make and map them to an integer, this is used to convert Blind Make to numerical. This is implemented using the categorical_transformer() function.\n",
    "\n",
    "2) Performance Metric Used\n",
    "\n",
    "Using a RMSE as a performance metric for this problem does not seem like a good measure. This is mainly due to the imbalanced claims (zero and non-zero). Even though the regression model can provide lower RMSE values, the tandem model would be the better model since it tries to classify the zero and non-zero claims and then perform regression on the non-zero claims, while the standalone regression model (step 2) might always try to predict a float value which may prevent it from giving a zero claim. From my understanding, the standalone regression model may not be able to capture zero and non-zero claim patterns, which could be vital for an insurance company.\n",
    "\n",
    "3) Undersampling\n",
    "\n",
    "Undersampling the majority class would be a better approcah to balance the given dataset since it is an unbalanced mix of zero (high) and non zero (low) claims. Oversampling the minority class (non zero claims) might produce inaccurate float values, which will alter the quality of the training data. Undersampling, to balance the zero and non-zero claims, significantly helped to improve the f1 score value of the binary classifier, in the tandem model, since classifiers are generally more biased towards detecting the majority class. It also gives more importance to the non-zero claims in the standalone regressor.\n",
    "\n",
    "4) Tandem Regressor vs Standalone Regressor (RMSE)\n",
    "\n",
    "The tandem model regressor gives higher RMSE value when compared to a standalone regressor. This could have arisen due to the limited amount of non-zero claim features used to train and validate the tandem model regressor. The high number of zero claims while training and validating the standalone regressor could have trained it to better predict zero claim values, thereby lowering the RMSE value on the validation set, which could have also contained high number of zero claims."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
